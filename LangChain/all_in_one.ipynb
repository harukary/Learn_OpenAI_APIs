{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,openai,json\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChainの主な機能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model｜呼び出しを簡単にしたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！どのようにお手伝いできますか？\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "def chat_completion(prompt, model=\"gpt-3.5-turbo\", temperature=0.0):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(model=model,messages=messages,temperature=temperature)\n",
    "    return response.choices[0].message[\"content\"]\n",
    "print(chat_completion('こんにちは。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "こんにちは！どのようにお手伝いできますか？\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "def chat_completion(prompt):\n",
    "    prompt = [HumanMessage(content='こんにちは。')]\n",
    "    response = chat(prompt)\n",
    "    return response.content\n",
    "print(chat_completion('こんにちは。'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template｜プロンプトを汎用的に使いたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='ドイツ語で「こんにちは。」は「Guten Tag.」と言います。' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "\n",
    "template_string = \"\"\"\\\n",
    "{language}で「{text}」は何と言いますか？\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "\tlanguage='ドイツ語',\n",
    "\ttext='こんにちは。'\n",
    ")\n",
    "\n",
    "print(chat(messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Parser｜出力を他の処理に使いたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"白菜\",\n",
      "  \"nutirition\": {\n",
      "    \"calories\": 13,\n",
      "    \"carbohydrates\": 2.2,\n",
      "    \"protein\": 1.2,\n",
      "    \"vitamin_c\": 45.0,\n",
      "    \"fiber\": 1.2\n",
      "  },\n",
      "  \"compatible_foods\": [\n",
      "    \"豚肉\",\n",
      "    \"にんじん\",\n",
      "    \"しいたけ\",\n",
      "    \"もやし\",\n",
      "    \"ごま油\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class Nutrition(BaseModel):\n",
    "    calories: int = Field(description='100gあたりカロリー（kcal）')\n",
    "    carbohydrates: float = Field(description='100gあたり炭水化物（g）')\n",
    "    protein: float = Field(description='100gあたりたんぱく質（g）')\n",
    "    vitamin_c: float = Field(description='100gあたりビタミンC（mg）')\n",
    "    fiber: float = Field(description='100gあたり食物繊維（g）')\n",
    "\n",
    "class Food(BaseModel):\n",
    "    name: str = Field(description='食材名')\n",
    "    nutirition: Nutrition = Field(description='食材の100gあたり栄養素')\n",
    "    compatible_foods: list[str] = Field(description='相性の良い食材（5つまで）')\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "parser = PydanticOutputParser(pydantic_object=Food)\n",
    "\n",
    "about_food_template = \"\"\"\n",
    "{food}について教えてください。相性の良い食材や100gあたりの栄養成分など\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template=about_food_template)\n",
    "\n",
    "food = '白菜'\n",
    "messages = prompt.format_messages(\n",
    "    food=food, format_instructions=parser.get_format_instructions()\n",
    ")\n",
    "response = chat(messages)\n",
    "food_data = parser.parse(response.content)\n",
    "print(json.dumps(food_data.dict(),indent=2,ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Memory｜記憶を持たせたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The human greets the AI with a \"hi\" and the AI responds by asking what's up.\n",
      "Human: not much you\n",
      "AI: not much\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=20)\n",
    "\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
    "\n",
    "memory_variables = memory.load_memory_variables({})\n",
    "print(memory_variables['history'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: こんにちは。元気？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'こんにちは！私はAIですので、元気ではありませんが、お話しできることが嬉しいです。いかがお過ごしですか？'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationSummaryBufferMemory(llm=ChatOpenAI(), max_token_limit=40),\n",
    "    verbose=True,\n",
    ")\n",
    "conversation_with_summary.predict(input=\"こんにちは。元気？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI in Japanese and asks how it is doing. The AI responds by saying it is an AI and cannot have feelings, but it is happy to be able to communicate. The AI then asks how the human is doing.\n",
      "Human: 今は勉強会の準備をしています。\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'勉強会の準備ですか？それは素晴らしいですね！どのような勉強会ですか？テーマや内容はありますか？'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"今は勉強会の準備をしています。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI in Japanese and asks how it is doing. The AI responds by saying it is an AI and cannot have feelings, but it is happy to be able to communicate. The AI then asks how the human is doing. The human mentions that they are preparing for a study group. The AI responds by saying that preparing for a study group is wonderful and asks about the theme and content of the study group.\n",
      "Human: LangChainです。LangChainのデモを作成しています。\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'こんにちは！LangChainのデモを作成しているんですね。それは素晴らしいです！LangChainとは、どのようなテーマや内容の勉強会なんでしょうか？詳しく教えていただけますか？'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"LangChainです。LangChainのデモを作成しています。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Chain｜連続で処理したい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "df = pd.read_csv('data/Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "# chain 1: input= Review and output= English_Review\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "chain_one = LLMChain(\n",
    "    llm=llm, prompt=first_prompt, output_key=\"English_Review\"\n",
    ")\n",
    "\n",
    "# chain 2: input= English_Review and output= summary\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "chain_two = LLMChain(\n",
    "    llm=llm, prompt=second_prompt, output_key=\"summary\"\n",
    ")\n",
    "\n",
    "# chain 3: input= Review and output= language\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "chain_three = LLMChain(\n",
    "    llm=llm, prompt=third_prompt, output_key=\"language\"\n",
    ")\n",
    "\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "chain_four = LLMChain(\n",
    "    llm=llm, prompt=fourth_prompt, output_key=\"followup_message\"\n",
    ")\n",
    "\n",
    "# overall_chain: input= Review and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\n",
      "  \"Review\": \"Je trouve le go\\u00fbt m\\u00e9diocre. La mousse ne tient pas, c'est bizarre. J'ach\\u00e8te les m\\u00eames dans le commerce et le go\\u00fbt est bien meilleur...\\nVieux lot ou contrefa\\u00e7on !?\",\n",
      "  \"English_Review\": \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\",\n",
      "  \"summary\": \"The reviewer regards the taste of the product as mediocre and suspects that it may be an old batch or counterfeit since the foam doesn't hold and the taste is inferior compared to the ones bought in stores.\",\n",
      "  \"followup_message\": \"R\\u00e9ponse de suivi:\\n\\nCher(e) critique, \\n\\nNous vous remercions d'avoir pris le temps de partager votre opinion concernant notre produit. Nous sommes d\\u00e9sol\\u00e9s d'apprendre que vous n'avez pas \\u00e9t\\u00e9 pleinement satisfait(e) de son go\\u00fbt. \\n\\nNous tenons \\u00e0 vous assurer que notre produit est authentique et que nous faisons tout notre possible pour maintenir un contr\\u00f4le rigoureux de la qualit\\u00e9. Cependant, il est possible que vous ayez re\\u00e7u un lot plus ancien qui a pu alt\\u00e9rer sa texture ou son go\\u00fbt. Nous vous pr\\u00e9sentons nos plus sinc\\u00e8res excuses pour cette exp\\u00e9rience d\\u00e9cevante.\\n\\nNous appr\\u00e9cions vos commentaires et nous prenons votre opinion en compte. Nous ferons en sorte d'en informer notre \\u00e9quipe de production afin qu'ils puissent mener des investigations approfondies pour \\u00e9valuer les probl\\u00e8mes que vous avez mentionn\\u00e9s. \\n\\nSoucieux de la satisfaction de nos clients, nous aimerions vous offrir un remplacement ou un remboursement complet pour votre achat. Veuillez s'il vous pla\\u00eet contacter notre service client\\u00e8le et leur fournir les d\\u00e9tails de votre achat, afin que nous puissions r\\u00e9soudre ce probl\\u00e8me \\u00e0 votre enti\\u00e8re satisfaction.\\n\\nEncore une fois, nous tenons \\u00e0 vous remercier pour votre retour et nous nous excusons sinc\\u00e8rement pour les d\\u00e9sagr\\u00e9ments que cela a pu causer. Nous esp\\u00e9rons avoir la possibilit\\u00e9 de vous servir \\u00e0 nouveau et de vous offrir une exp\\u00e9rience positive. \\n\\nCordialement,\\n\\nL'\\u00e9quipe du service client\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "review = df.Review[5]\n",
    "response = overall_chain(review)\n",
    "print(json.dumps(response,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectore Stores｜専門知識をもとに応答させたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# ===== CSV loader for Jester sentences in outdoor clothing dataset ====\n",
    "file = 'data/OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docs = loader.load()\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")\n",
    "retriever = db.as_retriever()\n",
    "llm = ChatOpenAI(temperature=0.0, model='gpt-4')\n",
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, here is a summary of the shirts with sun protection:\n",
       "\n",
       "| Shirt ID | Name | Description |\n",
       "| --- | --- | --- |\n",
       "| 618 | Men's Tropical Plaid Short-Sleeve Shirt | This shirt is made of 100% polyester and is wrinkle-resistant. It has a traditional fit and is relaxed through the chest, sleeve, and waist. It features front and back cape venting and two front bellows pockets. It provides UPF 50+ sun protection, blocking 98% of the sun's harmful rays. |\n",
       "| 255 | Sun Shield Shirt | This shirt is made of 78% nylon and 22% Lycra Xtra Life fiber. It is slightly fitted and falls at the hip. It wicks moisture for quick-drying comfort and fits comfortably over your favorite swimsuit. It is abrasion-resistant and provides UPF 50+ sun protection, blocking 98% of the sun's harmful rays. |\n",
       "| 374 | Men's Plaid Tropic Shirt, Short-Sleeve | This shirt is made of 52% polyester and 48% nylon. It is designed for fishing and is great for extended travel. It is wrinkle-free and quickly evaporates perspiration. It features front and back cape venting and two front bellows pockets. It provides UPF 50+ sun protection, blocking 98% of the sun's harmful rays. |\n",
       "| 535 | Men's TropicVibe Shirt, Short-Sleeve | This shirt is made of 71% Nylon and 29% Polyester. It has a traditional fit and is relaxed through the chest, sleeve, and waist. It is wrinkle-resistant and features front and back cape venting and two front bellows pockets. It provides UPF 50+ sun protection, blocking 98% of the sun's harmful rays. |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"List all your shirts with sun protection \\\n",
    "in a table with columns Shirt ID, Name, Description \\\n",
    "in markdown and summarize each one.\"\n",
    "response = qa_stuff.run(query)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation｜LLMアプリケーションを評価したい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "file = 'data/OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "docs = loader.load()\n",
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs, \n",
    "    embeddings\n",
    ")\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model='gpt-4')\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever,\n",
    "    verbose=True,\n",
    "    chain_type_kwargs = {\n",
    "        \"document_separator\": \"<<<<>>>>>\"\n",
    "    }\n",
    ")\n",
    "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix outpit parser\n",
    "from langchain.output_parsers import RegexParser\n",
    "output_parser = RegexParser(\n",
    "    regex=r\"QUESTION: (.*?)\\n{1,2}ANSWER: (.*)\", output_keys=[\"query\", \"answer\"]\n",
    ")\n",
    "example_gen_chain.prompt.output_parser = output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"query\": \"What are the key features of the Women's Campside Oxfords?\",\n",
      "  \"answer\": \"The key features of the Women's Campside Oxfords include a super-soft canvas material for a broken-in feel and look, a comfortable EVA innersole with Cleansport NXT\\u00ae antimicrobial odor control, a vintage hunt, fish, and camping motif on the innersole, a moderate arch contour of the innersole, an EVA foam midsole for cushioning and support, and a chain-tread-inspired molded rubber outsole with a modified chain-tread pattern.\"\n",
      "}\n",
      "\n",
      "{\n",
      "  \"query\": \"What are the dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave?\",\n",
      "  \"answer\": \"The dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\\\" x 28\\\".\"\n",
      "}\n",
      "\n",
      "{\n",
      "  \"query\": \"What are some features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece?\",\n",
      "  \"answer\": \"Some features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece include bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom, and the ability to machine wash and line dry.\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in docs[:3]]\n",
    ")\n",
    "for ex in examples:\n",
    "    print(json.dumps(ex,indent=2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Example 0:\n",
      "Question: What are the key features of the Women's Campside Oxfords?\n",
      "Real Answer: The key features of the Women's Campside Oxfords include a super-soft canvas material for a broken-in feel and look, a comfortable EVA innersole with Cleansport NXT® antimicrobial odor control, a vintage hunt, fish, and camping motif on the innersole, a moderate arch contour of the innersole, an EVA foam midsole for cushioning and support, and a chain-tread-inspired molded rubber outsole with a modified chain-tread pattern.\n",
      "Predicted Answer: The Women's Campside Oxfords have several key features. They are made from a soft canvas material that gives them a broken-in feel and look. They have a comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. The innersole also features a vintage hunt, fish, and camping motif and has a moderate arch contour. The shoes also have an EVA foam midsole for cushioning and support, and a chain-tread-inspired molded rubber outsole with a modified chain-tread pattern. They are imported and weigh approximately 1 lb.1 oz. per pair.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 1:\n",
      "Question: What are the dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave?\n",
      "Real Answer: The dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\" x 28\".\n",
      "Predicted Answer: The dimensions of the small size of the Recycled Waterhog Dog Mat, Chevron Weave are 18\" x 28\".\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 2:\n",
      "Question: What are some features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece?\n",
      "Real Answer: Some features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece include bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ rated fabric for sun protection, crossover no-slip straps, fully lined bottom, and the ability to machine wash and line dry.\n",
      "Predicted Answer: The Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece features bright colors, ruffles, and exclusive whimsical prints. It is made from a four-way-stretch and chlorine-resistant fabric that keeps its shape and resists snags. The fabric is also UPF 50+ rated, providing the highest rated sun protection possible and blocking 98% of the sun's harmful rays. The swimsuit has crossover no-slip straps and a fully lined bottom for a secure fit and maximum coverage. For best results, it should be machine washed and line dried.\n",
      "Predicted Grade: CORRECT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = qa.apply(examples)\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs = eval_chain.evaluate(examples, predictions)\n",
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents｜タスク計画・実行を自分でやってほしい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model='gpt-4')\n",
    "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)\n",
    "agent= initialize_agent(\n",
    "    tools,llm, \n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThought: This is a simple math problem. I can use the calculator tool to solve it.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"25% of 300\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 75.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "Final Answer: The 25% of 300 is 75.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent(\"What is the 25% of 300?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find out what book Tom M. Mitchell wrote. I can use Wikipedia to find this information.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Wikipedia\",\n",
      "  \"action_input\": \"Tom M. Mitchell\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: Tom M. Mitchell\n",
      "Summary: Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU). He is a founder and former Chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science and a Fellow and past President of the Association for the Advancement of Artificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science at Carnegie Mellon.\n",
      "\n",
      "Page: Tom Mitchell (Australian footballer)\n",
      "Summary: Thomas Mitchell (born 31 May 1993) is a professional Australian rules footballer playing for the Collingwood Football Club in the Australian Football League (AFL). He previously played for the Sydney Swans from 2012 to 2016, and the Hawthorn Football Club between 2017 and 2022. Mitchell won the Brownlow Medal as the league's best and fairest player in 2018 and set the record for the most disposals in a VFL/AFL match, accruing 54 in a game against Collingwood during that season.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe Wikipedia search for Tom M. Mitchell reveals that he is the author of the textbook \"Machine Learning\". This is the book he wrote. \n",
      "Final Answer: Tom M. Mitchell wrote the textbook \"Machine Learning\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"Tom M. Mitchell is an American computer scientist \\\n",
    "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
    "what book did he write?\"\n",
    "result = agent(question) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
